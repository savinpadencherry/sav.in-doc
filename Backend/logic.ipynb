{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa9e7ba",
   "metadata": {},
   "source": [
    "IMPORTS AND LLM INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac68d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "from gradio_pdf import PDF\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "CHATS_DIR = \"./chats\"\n",
    "#defines a directory to store chat history\n",
    "os.makedirs(CHATS_DIR, exist_ok=True)\n",
    "\n",
    "LLM_MODEL = \"deepseek-r1:1.5b\"\n",
    "EMBED_MODEL = \"granite-embedding:278m\"\n",
    "\n",
    "llm = OllamaLLM(model=LLM_MODEL, temperature=0.5)\n",
    "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497957bc",
   "metadata": {},
   "source": [
    "LOADING PREVIOUS CHAT=> chat_dict -> checking everything in os.listdir(chats_dir) -> if ends with .json -> remove the .json and store in chat_id -> open the file as read only with open(chats_dir, filename) and store it in f -> json.load(f) and put it in data -> for chat[chat_id] = data put that data in and return the chats, create a flow diagram image for this please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e816a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chats():\n",
    "    chat = {}\n",
    "    #for every filename in listdir with CHATS_DIR\n",
    "    for filename in os.listdir(CHATS_DIR):\n",
    "        #if filename ends with .json, load the file and add it to chat dictionary\n",
    "        if filename.endswith(\".json\"):\n",
    "            #this remove the .json extension to use as chat_id\n",
    "            chat_id = filename[:-5]\n",
    "            #os.path.join combine the ./chats/chat_id.json and opens it as read only and\n",
    "            #assigns it variable f \n",
    "            with open(os.path.join(CHATS_DIR, filename), 'r') as f:\n",
    "                #loads the json file and assigns it into a python dictionary\n",
    "                data = json.load(f)\n",
    "            #adds the chat_id as key and data as value to the chat dictionary    \n",
    "            chat[chat_id] = data\n",
    "    return chat\n",
    "#loading the chats on to the variable and making it available for the UI\n",
    "chats = load_chats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78329264",
   "metadata": {},
   "source": [
    "Create New CHAT => flow  -> create a new chat_id by taking length +1 , Building a path using os.path.join(chats_dir, chatid) -> creating the path to to store using os.makedirs(chat_dir, exist_ok = True) -> then creating the chat with the id -> save_chat function returns chat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1068ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_chat(chat_name = \"New Chat\"):\n",
    "    #creating a new chat id by checking the length of existing chats +1\n",
    "    chat_id = f\"chat_{len(chats) + 1}\"\n",
    "    #Building a full path for a new sub-directory\n",
    "    #chroma needs a unique folder for each chat \n",
    "    chat_dir = os.path.join(CHATS_DIR, chat_id)\n",
    "    # creating a path with the chat_dir\n",
    "    os.makedirs(chat_dir, exist_ok=True)\n",
    "    #creating a new chat with the chat_id and chat_name\n",
    "    chats[chat_id] = {\n",
    "        \"name\":chat_name,\n",
    "        \"history\": [],\n",
    "        \"vector_dir\":chat_dir,\n",
    "        \"pdf_path\":None,\n",
    "        \"memory\": ConversationBufferMemory()\n",
    "    }\n",
    "\n",
    "    save_chat(chat_id)\n",
    "    return chat_id\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6392f",
   "metadata": {},
   "source": [
    "SAVE_CHAT -> create a copy of the chat -> remove memory -> open the path using os.path.join of chats dir and id json -> Dump the data in Json format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7c88ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chat(chat_id):\n",
    "    #grab the chats dict from global\n",
    "    #access the chat via id\n",
    "    #copy it into data to make modification and then store/replace\n",
    "    data = chats[chat_id].copy()\n",
    "    #so in memory we might store ConversationBufferMemory/ConversationSummaryMemory etc\n",
    "    # Now these are langchain objects not python objects so you will get an error\n",
    "    # to fight this error you remove it \n",
    "    data.pop(\"memory\", None)\n",
    "    #Open a file for writing the data\n",
    "    #Join the chat dir and id with w \n",
    "    #B\n",
    "    with open(os.path.join(CHATS_DIR, f\"{chat_id}.json\"), \"w\") as f:\n",
    "        json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615824e",
   "metadata": {},
   "source": [
    "DELETE CHAT -> os.remove(path) -> remove from vector db using shutil.rmtree -> del chats[chat_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "96e87d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def delete_chat(chat_id): \n",
    "    #build the json file path and remove it with os.remove\n",
    "    os.remove(os.path.join(CHATS_DIR, f\"{chat_id}.json\"))\n",
    "    #deleles the entire vector directory using shutil.rmtree\n",
    "    shutil.rmtree(chats[chat_id][\"vector_dir\"], ignore_errors=True)\n",
    "    #deletes entry from the global chat dic\n",
    "    del chats[chat_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd5607",
   "metadata": {},
   "source": [
    "Document loading -> Splitting -> Embeddings and storage  and Updating the chat path to realise pdf for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b2352322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_document(chat_id, file_path):\n",
    "    try:\n",
    "    #this is document loader to extract text/pages\n",
    "     loader = PyPDFLoader(file_path)\n",
    "     #loads the pdf into a list of documents, each representing a page or chunk\n",
    "     #using REcurvsiveTextSplitter to split the text \n",
    "     doc = loader.load_and_split(text_splitter= RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50))\n",
    "     # create a chroma vector store from the chunked docs. from_documents embeds each doc using \n",
    "     # use global embeddings in store vectors in chroma \n",
    "     # persist_directory points to the chat's vector directory\n",
    "     vectorstore = Chroma.from_documents(doc, embeddings, persist_directory=chats[chat_id]['vector_dir'])\n",
    "     #save the vector store to disk in the specified directory as done above\n",
    "     vectorstore.persist()\n",
    "     #Update thr global dict for this ID , setting \"PDf_path\" to the file path\n",
    "     #so we can view this pdf later on\n",
    "     chats[chat_id][\"pdf_path\"] = file_path\n",
    "     #saved the chat to presist\n",
    "     save_chat(chat_id)\n",
    "    except Exception as e:\n",
    "        return f\"Error uploading: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f76ddd",
   "metadata": {},
   "source": [
    "SETUP RAG CHAIN -> Load vector store, create a retriever and recreaes and load the memory with past history , set a prompt template and build retrievalQA chain -> Chain + Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "59379d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_chain(chat_id):\n",
    "    #load the existing chroma vector store from the chat's saved directory, no re-embedding \n",
    "    vectorstore = Chroma(persist_directory=chats[chat_id][\"vector_dir\"], embedding_function=embeddings)\n",
    "    #turn the vector store into a retriever  so it can search for relevant chunks \n",
    "    retriever = vectorstore.as_retriever()\n",
    "    #Create a memory object \n",
    "    memory = ConversationBufferMemory()\n",
    "    #starts a for loop over the chat history list , to replay the entire conversation history into memory\n",
    "    for msg in chats[chat_id][\"history\"]:\n",
    "        memory.save_context({\"input\":msg[\"user\"]}, {\"output\":msg[\"bot\"]})\n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"],template=\"\"\"\n",
    "    You are **sav.in**, a helpful assistant.\n",
    "\n",
    "    ðŸ” Step 1: Read the context below and answer based **only** on that.\n",
    "    If the answer cannot be found in the context, proceed to Step 2.\n",
    "\n",
    "    Step 2: Answer from your general knowledge, but clearly indicate you're using that knowledge.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ðŸ“„ Context:\n",
    "    {context}\n",
    "\n",
    "    â“ User Question:\n",
    "    {question}\n",
    "\n",
    "    ---\n",
    "\n",
    "    ðŸ’¬ Answer as sav.in:\n",
    "    \"\"\")\n",
    "    chain = RetrievalQA.from_chain_type(llm = llm, chain_type = \"stuff\", retriever = retriever, memory = memory, return_source_documents = True)\n",
    "    return chain,memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29d8f2",
   "metadata": {},
   "source": [
    "QUERY_CHAT =>. get the  chain by get_rag_chain => executive the chain by sending a query => store the result from results['results'] in response and sources = results['source_documents']  => Append the new response into chat history by chats[chat_id][\"history\"].append({\"user\": query, \"bot\": response}) => save chat in disk => get the highlight text = [doc.page_content for doc in sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a16f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chat(chat_id, query):\n",
    "    #check if chat exists in chat and has a PDF uploaded \n",
    "    #prevent queries without a document \n",
    "    if chat_id not in chats or not chats[chat_id]['pdf_path']:\n",
    "        #for UI to render\n",
    "        return \"Upload a document first.\"\n",
    "    #sets up rag pipeline with loaded data/memory\n",
    "    chain, memory = get_rag_chain(chat_id)\n",
    "    #sending query into the retrievalqa chain and storing the result in result\n",
    "    result = chain({\"query\": query})\n",
    "    #extracts the LLM generated answer from result in response\n",
    "    response = result[\"result\"]\n",
    "    #for highlights it will return a list of chunks to highlight in PDF\n",
    "    sources = result['source_documents']\n",
    "    #append the query and response to history to record it and store it in disk\n",
    "    chats[chat_id][\"history\"].append({\"user\":query, \"bot\":response})\n",
    "    #save the chat for updated history\n",
    "    save_chat(chat_id)\n",
    "    #for each doc in sources get the chunk text which will be present in page_content\n",
    "    #Prepares the string for PDF search / Highlight\n",
    "    highlight_texts = [doc.page_content for doc in sources]\n",
    "    #return the response and highlight texts\n",
    "    return response, highlight_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "078b029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_lottie():\n",
    "    return \"\"\"\n",
    "    <script src=\"https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js\"></script>\n",
    "    <lottie-player src=\"assets/survey.json\" background=\"transparent\" speed=\"1\" style=\"width: 300px; height: 300px;\" loop autoplay></lottie-player>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad80ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/60/pv34rdp10rb2czvb42srqffh0000gn/T/ipykernel_4010/2687175571.py:46: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/blocks.py\", line 2239, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/blocks.py\", line 1966, in postprocess_data\n",
      "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/blocks.py\", line 1921, in validate_outputs\n",
      "                raise ValueError(\n",
      "    ...<5 lines>...\n",
      "                )\n",
      "ValueError: A  function (create_new_chat) didn't return enough output values (needed: 2, returned: 1).\n",
      "    Output components:\n",
      "        [dropdown, state]\n",
      "    Output values returned:\n",
      "        [\"chat_1\"]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/blocks.py\", line 2239, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/blocks.py\", line 2021, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/components/chatbot.py\", line 633, in postprocess\n",
      "    self._check_format(value, \"tuples\")\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/savinpadencherry/development/projects/sav.in/myvenv/lib/python3.13/site-packages/gradio/components/chatbot.py\", line 430, in _check_format\n",
      "    raise Error(\n",
      "        \"Data incompatible with tuples format. Each message should be a list of length 2.\"\n",
      "    )\n",
      "gradio.exceptions.Error: 'Data incompatible with tuples format. Each message should be a list of length 2.'\n"
     ]
    }
   ],
   "source": [
    "def render_lottie():\n",
    "    return \"\"\"\n",
    "    <script src=\"https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js\"></script>\n",
    "    <lottie-player src=\"assets/chat_lottie.json\" background=\"transparent\" speed=\"1\" style=\"width: 300px; height: 300px;\" loop autoplay></lottie-player>\n",
    "    \"\"\"\n",
    "\n",
    "theme = gr.themes.Monochrome(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"purple\",\n",
    "    neutral_hue=\"slate\",\n",
    ").set(\n",
    "    body_background_fill=\"linear-gradient(to bottom, #0F172A, #1E293B)\",\n",
    "    body_background_fill_dark=\"linear-gradient(to bottom, #0F172A, #1E293B)\",\n",
    "    button_primary_background_fill=\"linear-gradient(90deg, #1E3A8A, #6D28D9)\",\n",
    "    button_primary_background_fill_hover=\"linear-gradient(90deg, #1D4ED8, #5B21B6)\",\n",
    "    block_radius=\"*radius_xl\",\n",
    ")\n",
    "\n",
    "custom_css = \"\"\"\n",
    "body { \n",
    "    background: repeating-linear-gradient(45deg, transparent, transparent 10px, rgba(255,255,255,0.05) 10px, rgba(255,255,255,0.05) 20px);\n",
    "    background-color: #0F172A;\n",
    "    animation: twinkle 3s infinite alternate;\n",
    "}\n",
    "@keyframes twinkle { 0% { opacity: 0.8; } 100% { opacity: 1; } }\n",
    ".file { border: 2px dashed #6D28D9; transition: all 0.3s; animation: fadeIn 0.5s; }\n",
    ".file:hover { border: 2px solid #A855F7; box-shadow: 0 0 15px #6D28D9; animation: pulse 1s infinite; }\n",
    "@keyframes pulse { 0% { transform: scale(1); } 50% { transform: scale(1.05); } 100% { transform: scale(1); } }\n",
    ".chatbot, .pdf { box-shadow: 0 4px 8px rgba(0,0,0,0.2); border-radius: 15px; background: linear-gradient(to bottom, #1E293B, #0F172A); animation: fadeIn 0.5s; }\n",
    ".column:first-child { background: linear-gradient(to right, #0F172A, #1E293B); }\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(title=\"Sav.in Document Query\", theme=theme, css=custom_css) as app:\n",
    "    gr.Markdown(\"# Sav.in Query Chatbot\", elem_classes=\"cosmic-header\")\n",
    "    current_chat = gr.State(None)\n",
    "    chats_list = gr.State(chats)\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            chat_dropdown = gr.Dropdown(label=\"Select Chat\", choices=list(chats.keys()))\n",
    "            new_chat_btn = gr.Button(\"Spawn New Chat\")\n",
    "            delete_btn = gr.Button(\"Erase and Delete Chat\")\n",
    "        with gr.Column(scale=4):\n",
    "            welcome = gr.HTML(render_lottie() + \"<h2 style='text-align: center; color: #A855F7;'>Ignite your first cosmic query</h2>\", visible=len(chats) == 0)\n",
    "            with gr.Row(visible=False) as chat_row:\n",
    "                with gr.Column(scale=2):\n",
    "                    chatbot = gr.Chatbot(height=400)\n",
    "                    msg = gr.Textbox(label=\"Ask your query\", interactive=False)\n",
    "                    upload = gr.File(label=\"Drag PDF on Here\", type=\"filepath\", interactive=True)\n",
    "                    clear = gr.Button(\"Reset Chat\")\n",
    "                    save_note_btn = gr.Button(\"Capture Response as Note\")\n",
    "                pdf_viewer = PDF(label=\"Document View\", interactive=True, height=400)\n",
    "            notes = gr.TextArea(label=\"Notes\", visible=False)\n",
    "\n",
    "    def update_chats():\n",
    "        return gr.update(choices=list(chats.keys())), chats  # Fixed typo: .ley() -> .keys()\n",
    "\n",
    "    new_chat_btn.click(create_new_chat, outputs=[chat_dropdown, chats_list]).then(update_chats, outputs=[chat_dropdown, chats_list])\n",
    "  \n",
    "    def select_chat(chat_id):\n",
    "        #processes selection\n",
    "        if not chat_id:\n",
    "            #if no id then returns the updates : show welcome and hide chat row, disable input and clear\n",
    "            return gr.update(visible=True), gr.update(visible=False), gr.update(interactive=False), None, None\n",
    "        #set states to current chat \n",
    "        current_chat.value = chat_id\n",
    "        #Builds list of user/bot interactions from history\n",
    "        history = [[(item[\"user\"], item[\"bot\"]) for item in chats[chat_id][\"history\"]]]\n",
    "        #gets saved pdf path, loadds into viewer\n",
    "        pdf_path = chats[chat_id][\"pdf_path\"]\n",
    "        #Activate cht Ui\n",
    "        return gr.update(visible=False), gr.update(visible=True), gr.update(interactive=True), history, pdf_path\n",
    "    #Triggers on selection\n",
    "    chat_dropdown.change(select_chat, inputs=chat_dropdown, outputs=[welcome, chat_row, msg, chatbot, pdf_viewer])\n",
    "    \n",
    "    def add_message(history, message):\n",
    "        #Updates display, and takes current history list and user message \n",
    "        history.append((message, None))\n",
    "        return history, \"\"\n",
    "    \n",
    "    def bot_response(history, chat_id):\n",
    "        #extracts user query for rag call\n",
    "        query = history[-1][0]\n",
    "        # Calls query function. gets answer and chunks \n",
    "        response, highlights = query_chat(chat_id, query)\n",
    "        # replaces placeholder with response\n",
    "        history[-1] = (query, response)\n",
    "        # returns history, updates PDF search with joined highlight\n",
    "        return history, gr.update(search_text=\"|\".join(highlights))\n",
    "    #chat interaction loop, binds submit to add message , input si current history and message \n",
    "    msg.submit(add_message, [chatbot, msg], [chatbot, msg]).then(bot_response, [chatbot, current_chat], [chatbot, pdf_viewer])\n",
    "    #Binds change to upload document \n",
    "    upload.change(upload_document, [current_chat, upload], None)\n",
    "    # deltes chat \n",
    "    delete_btn.click(delete_chat, chat_dropdown).then(update_chats, outputs=[chat_dropdown, chats_list])\n",
    "    \n",
    "    def save_note(history):\n",
    "        if history:\n",
    "            #Gets last bot response , why and what to save \n",
    "            last_resp = history[-1][1]\n",
    "            #\n",
    "            with open(\"notes.txt\", \"a\") as f:\n",
    "                f.write(last_resp + \"\\n\\n\")\n",
    "            return \"Echo captured!\"\n",
    "    \n",
    "    save_note_btn.click(save_note, chatbot, notes)\n",
    "    \n",
    "    clear.click(lambda: [], None, chatbot)\n",
    "\n",
    "app.launch(inline=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
